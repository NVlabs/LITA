# Copyright (c) 2024, NVIDIA Corporation & Affiliates. All rights reserved. 
# 
# This work is made available under the Nvidia Source Code License-NC. 
# To view a copy of this license, visit 
# https://github.com/NVlabs/LITA/blob/main/LICENSE

import torch

from llava.utils import disable_torch_init
from llava.conversation import SeparatorStyle
from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria

from lita.model.builder import load_pretrained_model
from lita.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, TIME_TOKEN_TEMPLATE

from transformers import TextStreamer


title_markdown = ("""
# LITA: Language Instructed Temporal-localization Assistant
[[Paper]](https://arxiv.org/abs/2403.19046) [[Code]](https://github.com/NVlabs/LITA)
""")

block_css = """
#buttons button {
    min-width: min(120px,100%);
}
"""


tos_markdown = ("""
### Terms of use
By using this service, users are required to agree to the following terms:
The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. The service may collect user dialogue data for future research.
For an optimal experience, please use desktop computers for this demo, as mobile devices may compromise its quality.
""")


learn_more_markdown = ("""
### License
The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.
""")


class Chat:
    def __init__(self, model_path, model_base=None, conv_mode="llava_v1", load_8bit=False, load_4bit=False, device='cuda'):
        disable_torch_init()
        model_name = get_model_name_from_path(model_path)
        tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit)
        self.tokenizer = tokenizer
        self.model = model
        self.image_processor = image_processor
        self.conv_mode = conv_mode
        self.device = self.model.device
        
    @torch.inference_mode()
    def generate(self, image_tensor, inp, first_run, state, vid_len):
        
        if first_run:
            inp = DEFAULT_IMAGE_TOKEN + '\n' + inp
            
        state.append_message(state.roles[0], inp)
        state.append_message(state.roles[1], None)
        prompt = state.get_prompt()
        
        # generation
        tokenizer = self.tokenizer
        model = self.model
        
        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
        stop_str = state.sep if state.sep_style != SeparatorStyle.TWO else state.sep2
        keywords = [stop_str]
        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)    
        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        with torch.inference_mode():
            output_ids = model.generate(
                input_ids,
                images=image_tensor.unsqueeze(0).half().cuda(),
                do_sample=True,
                temperature=0.2,
                max_new_tokens=1024,
                streamer=streamer,
                use_cache=True,
                stopping_criteria=[stopping_criteria])
            
        def decode(out_ids):
            out = tokenizer.decode(out_ids).strip()
            if out.endswith(stop_str):
                out = out[:-len(stop_str)]
            out = out.strip()
            return out
        
        # raw outputs
        output_ids = output_ids[0, input_ids.shape[1]:]
        outputs = decode(output_ids)
        state.messages[-1][-1] = outputs
        print("\n", {"prompt": prompt, "outputs": outputs}, "\n")
        
        # time tokens decoded output
        num_time_tokens = model.config.num_time_tokens
        time_tokens = [TIME_TOKEN_TEMPLATE.format(t=x) for x in range(num_time_tokens)]
        time_token_ids = tokenizer.convert_tokens_to_ids(time_tokens)
        
        indexes = [j for j in range(len(output_ids) - 1) if output_ids[j] in time_token_ids]
        last_processed = -1
        new_output_ids = []
        duration = vid_len
        for j in range(len(indexes)):
            pred_seq = [int(output_ids[k]) for k in range(last_processed + 1, indexes[j])]
            new_output_ids.extend(pred_seq)
            
            max_offset = num_time_tokens - 1
            time_token = tokenizer.decode(output_ids[indexes[j]])
            time_idx = time_tokens.index(time_token)
            time = float(time_idx) * duration / max_offset
            time = min(max(time, 0), duration)
            time = round(time, 2)
            time_str = str(time) + 's'
            new_output_ids.extend(tokenizer.encode(time_str, add_special_tokens=False))
            
            last_processed = indexes[j]
            
        pred_seq = [int(x) for x in output_ids[last_processed + 1:]]
        new_output_ids.extend(pred_seq)
        # outputs = tokenizer.batch_decode([new_output_ids], skip_special_tokens=True)[0]
        outputs_timestamp = decode(new_output_ids)
            
        return state, outputs_timestamp
        
    
